#!/usr/bin/env bash
# torque/compute-execute -- Executes a process remotely using the Torque scheduler
# $ compute-execute input_sql=... command=... output_relation=...
#
# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES
# environment or the 'deepdive.computers.local.num_processes' in
# computers.conf:
# $ export DEEPDIVE_NUM_PROCESSES=2
# $ compute-execute input_sql=... command=... output_relation=...
##
set -euo pipefail

. commons

: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}

# load compute configuration
eval "$(jq2sh <<<"$DEEPDIVE_COMPUTER_CONFIG" \
    num_processes='.num_processes' \
    ssh_user='.ssh_user' \
    ssh_host='.ssh_host' \
    remote_deepdive_app_base='.remote_deepdive_app_base' \
    remote_deepdive_cwd_base='.remote_deepdive_cwd_base' \
    poll_period_secs='.poll_period_secs' \
    excludes='.excludes | join("\t")' \
    #
)"
# respect the DEEPDIVE_NUM_PROCESSES environment
num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(
        # detect number of processor cores
        nproc=$(
            # Linux typically has coreutils which includes nproc
            nproc ||
            # OS X
            sysctl -n hw.ncpu ||
            # fall back to 1
            echo 1
        )
        if [[ $nproc -gt 1 ]]; then
            # leave one processor out
            let nproc-=1
        elif [[ $nproc -lt 1 ]]; then
            nproc=1
        fi
        echo $nproc
    )}}

# declare all input arguments
declare -- "$@"

export SSH_INFO="${ssh_user}@${ssh_host}"
export LOCAL_USER="$(whoami)"

# App directory variables
# APP_ID is used to isolate app folders that are uploaded to the same path
# in the remote node. It is a hash of the local user name and the local absolute 
# DeepDive application path. 
export APP_ID="$(hash "$LOCAL_USER + $DEEPDIVE_APP")"
#SESSION_ID="$(date +%Y%m%d/%H%M%S.%N)"
export SESSION_ID="$DEEPDIVE_RUN_ID/$DEEPDIVE_CURRENT_PROCESS_NAME"

# Either take the remote DeepDive app path specified by the user, or default to
# remote dir. The app base directory is where DeepDive will create a new directory
# called dd_app, and all apps will be copied to this directory.
remote_deepdive_app_base=${remote_deepdive_app_base:-${remote_deepdive_cwd_base}}
remote_deepdive_app="$remote_deepdive_app_base/dd_app/$APP_ID"

# Current working directory variables.
# TODO: Change "run" to include some session ID to allow multiple processes
# running.
remote_deepdive_cwd="$remote_deepdive_cwd_base/dd_run/$APP_ID/$SESSION_ID"
export REMOTE_SH_DIR="$remote_deepdive_cwd/remote.sh"
# TODO: Decoupling input to reduce data transfers. 
export REMOTE_IN_DIR="$remote_deepdive_cwd/remote.in"
export REMOTE_OUT_DIR="$remote_deepdive_cwd/remote.out"
export REMOTE_ERR_DIR="$remote_deepdive_cwd/remote.err"
export REMOTE_JOBID_DIR="$remote_deepdive_cwd/job.id"

# show configuration
echo "Executing with the following configuration:"
echo " num_processes=$num_processes"
echo " ssh_user=$ssh_user"
echo " ssh_host=$ssh_host"
echo " remote_deepdive_app_base=$remote_deepdive_app_base"
echo " remote_deepdive_cwd_base=$remote_deepdive_cwd_base"
echo " poll_period_secs=$poll_period_secs"
echo " excludes=$excludes"
echo ""
echo "DEEPDIVE_APP = $DEEPDIVE_APP"
echo "DEEPDIVE_CURRENT_PROCESS_NAME = $DEEPDIVE_CURRENT_PROCESS_NAME"
echo "DEEPDIVE_COMPUTER_CONFIG = $DEEPDIVE_COMPUTER_CONFIG"
echo "DEEPDIVE_COMPUTER_TYPE = $DEEPDIVE_COMPUTER_TYPE"
echo ""
echo "Session information"
echo " application id : $LOCAL_USER + $DEEPDIVE_APP -> $APP_ID"
echo " session id     : $DEEPDIVE_CURRENT_PROCESS_NAME -> $APP_ID/$SESSION_ID"

# ensure all the directories needed exist in the remote node. 
ssh $SSH_INFO bash -c "'
  mkdir -p $(dirname $remote_deepdive_app)
  mkdir -p $remote_deepdive_cwd
'"

# Slash at the end of DEEPDIVE_APP/ is important, so that rsync doesn't create
# another folder inside $remote_deepdive_app
RSYNC_EXCLUDES=""
for exclude in $excludes; do
  # XXX: For some reason, if I enclose $exclude in double quotes and pass it
  # to rsync, rsync will interpret it as '"$exclude"', and fail to find 
  # $exclude.
  RSYNC_EXCLUDES+="--exclude $exclude "
done
echo "Copying deepdive app to remote node."
echo "Excluding $RSYNC_EXCLUDES"
rsync -aH $RSYNC_EXCLUDES --progress $DEEPDIVE_APP/ $SSH_INFO:$remote_deepdive_app

# Prepare input data
if [[ -n $input_sql ]]; then
  # Upload SQL data to the submission node
  deepdive-sql eval "$input_sql" format="$DEEPDIVE_LOAD_FORMAT" |
  ssh $SSH_INFO "cat > $REMOTE_IN_DIR"
fi

# Prepare submission script
# XXX there are conditional branches below depending on whether input_sql
# and/or output_relation is given, to support four use cases. Depending on
# the cases, need to generate various submission scripts. 
# 1) executing command while streaming data from/to the database
# 2) input-only command which has no output to the database and streams from the database
# 3) output-only command which has no input from the database and streams to the database
# 4) database-independent command which simply runs in parallel
# XXX: Right now, submission script doesn't handle the case when there's no 
# input_sql.
ssh_with_env \
  DEEPDIVE_APP=$remote_deepdive_app \
  NUM_PROCESSES=$num_processes \
  DEEPDIVE_CURRENT_PROCESS_NAME \
  REMOTE_IN_DIR \
  REMOTE_OUT_DIR \
  REMOTE_ERR_DIR \
  REMOTE_SH_DIR \
  -- "deepdive compute remote-taskinit"

# Ensure Kerberos authentication is succesful for user. Apparently this is 
# only needed for Mac OS X due to broken implementation of Kerberos. 
if [[ $(uname) = "Darwin" ]]; then
  KRB_DIR="/tmp/krb5cc_${ssh_user}_deepdive"
  if ! ssh $SSH_INFO "klist -s -c $KRB_DIR > /dev/null"; then
    echo "Kerberos credentials either expired or not found in remote node."
    echo "Please reenter password for $SSH_INFO."
    ssh -t $SSH_INFO "kinit -r 30d -c $KRB_DIR"
  else 
    echo "Kerberos credentials found at $SSH_INFO:$KRB_DIR"
  fi
fi

# Submit job. 
# This is a hack that supposedly prints out the output of ssh asking for the 
# password. But the output seems to not be flushed to stdout. 
JOB_ID=$(ssh_with_env \
  DEEPDIVE_APP=$remote_deepdive_app \
  SSH_HOST=$ssh_host \
  REMOTE_JOBID_DIR \
  REMOTE_SH_DIR \
  -- "deepdive compute remote-submit")

for sig in TERM INT; do
  trap "sched_kill $JOB_ID" $sig
done
# Poll status
echo "Waiting for job id $JOB_ID to complete"
STATUS=incomplete
#while [[ "$(compute_status)" == "Q" ]] && ssh -q $SSH_INFO "[[ ! -f $REMOTE_OUT_DIR ]]"; do
while [[ "$STATUS" == incomplete ]]; do
  echo "Waiting for $poll_period_secs seconds..."
  sleep $poll_period_secs

  set -x
  # Efficient way to compute the status as it lifts the burden of checking each
  # job in the submission node. 
  JOB_STATUSES=$(ssh_with_env \
    DEEPDIVE_APP=$remote_deepdive_app \
    JOB_ID \
    NUM_PROCESSES \
    REMOTE_OUT_DIR \
    -- "deepdive compute remote-status")
  
  echo $JOB_STATUSES
  # Funny way to count number of jobs. 
  NUM_COMPLETED=$(grep -o "C" <<< "$JOB_STATUSES" | grep -c "C" || :)
  NUM_INCOMPLETE=$(grep -o "I" <<< "$JOB_STATUSES" | grep -c "I" || :)
  NUM_FAILED=$(grep -o "F" <<< "$JOB_STATUSES" | grep -c "F" || :)

  if [[ $NUM_INCOMPLETE -eq 0 ]]; then
    if [[ $NUM_FAILED -gt 0 ]]; then
      STATUS=failed
    else
      STATUS=complete
    fi
  fi
done

if [[ "$STATUS" == failed ]]; then
    echo "Job failed!"
    echo "Please inspect $ssh_host:$REMOTE_ERR_DIR for more information"
    echo "----------------------------------------------------------------"
    ssh $SSH_INFO "tail +1 $REMOTE_ERR_DIR*" 
    echo "----------------------------------------------------------------"

    if [[ $num_processes -gt 1 ]]; then
        echo "--- Multi-job report ---"
        echo "Completed jobs: $NUM_COMPLETED"
        echo "Failed jobs: $NUM_FAILED"
        echo "Incomplete jobs: $NUM_INCOMPLETE"
    fi
    error
fi

# prepare a temporary output table when output_relation is given
if [[ -n $output_relation ]]; then
    # some derived values
    output_relation_tmp="${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}"

    # show configuration
    echo " output_relation_tmp=$output_relation_tmp"
    echo

    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation
    deepdive-create table-if-not-exists "$output_relation"
    db-create-table-like "$output_relation_tmp" "$output_relation"

    # Stream output from remote submission node directly to stdin, and to 
    # deepdive load
    if [[ $num_processes -gt 1 ]]; then
        extract="ssh $SSH_INFO \"cat $REMOTE_OUT_DIR-*\""
    else
        echo "Single process!!"
        extract="ssh $SSH_INFO \"cat $REMOTE_OUT_DIR\""
    fi
    echo $extract
    eval "$extract" > /dev/stdout |
    
    # use mkmimo again to merge outputs of multiple processes into a single stream
    #mkmimo process-*.output \> /dev/stdout |

    # load the output data to the temporary table in the database
    # XXX hiding default progress bar from deepdive-load
    # TODO abbreviate this env into a show_progress option, e.g., recursive=false
    show_progress input_to "$DEEPDIVE_CURRENT_PROCESS_NAME output" -- \
    env DEEPDIVE_PROGRESS_FD=2 \
    deepdive-load "$output_relation_tmp" /dev/stdin

    # rename the new temporary table
    # TODO maybe use PostgreSQL's schema support here?
    echo "Replacing $output_relation with $output_relation_tmp"
    output_relation_old="${DEEPDIVE_PREFIX_TABLE_OLD}${output_relation}"
    deepdive-sql "DROP TABLE IF EXISTS ${output_relation_old};" || true
    deepdive-sql "ALTER TABLE ${output_relation}     RENAME TO ${output_relation_old};"
    deepdive-sql "ALTER TABLE ${output_relation_tmp} RENAME TO ${output_relation};"
    deepdive-sql "DROP TABLE IF EXISTS ${output_relation_old};" || true
    # and analyze the table to speed up future queries
    db-analyze "${output_relation}"
fi
